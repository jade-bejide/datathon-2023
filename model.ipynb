{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e248b717",
   "metadata": {},
   "source": [
    "# BDSS Datathon 2023\n",
    "\n",
    "## Team name: work in progress\n",
    "\n",
    "### Jupyter Notebook\n",
    "\n",
    "Python can be run on [Jupyter Notebook](http://jupyter.org/) too.\n",
    "\n",
    "Jupyter Notebook is a computing environment supporting various programing languages (Python, R, Lua, etc.) through the concept of kernels.  \n",
    "It allows you to enrich your code with complex comments formatted in Markdown and $\\LaTeX$, as well as to place the results of your computation right below your code. Beside, it has all the features provided by the ipython interpreter, like tab auto-completion. \n",
    "\n",
    "Jupyter Notebook runs as a web server. To run this lab sheet navigate to the folder containing the file `labsheet1.ipynb` and run Jupyter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb2c67",
   "metadata": {},
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e687373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "# notebook\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = (16.0, 12.0)\n",
    "pylab.rcParams['font.size'] = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32bffe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import xgboost as xgb\n",
    "from scipy.stats import norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860cfb4",
   "metadata": {},
   "source": [
    "### Feature Selection and Visualisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d43d803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Generating Visualisations per class:'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads in csv files and returns data of the type DataFrame\n",
    "casualty_data = pd.read_csv(\"casualty_train.csv\", delimiter=\",\")\n",
    "vehicle_data = pd.read_csv(\"vehicle_train.csv\", delimiter=\",\")\n",
    "\n",
    "''' Merge Dataframes with a database-style join on the label \"accident_reference\"\n",
    "and a merge of type \"outer\" (similar to a SQL full outer join) '''\n",
    "# Note: the label \"accident_reference\" is common to both csv files\n",
    "\n",
    "# all_data of the type 'Dataframe' \n",
    "all_data = pd.merge(casualty_data, vehicle_data, on='accident_reference', how='outer')\n",
    "\n",
    "# checking structure of 'all_data'\n",
    "\n",
    "'''\n",
    "print(all_data.columns)\n",
    "print(len(all_data.columns))\n",
    "print(list(all_data.columns))\n",
    "'''\n",
    "\n",
    "# print(\"Features\")\n",
    "# print(casualty_data.columns)\n",
    "# print(\"=====\")\n",
    "\n",
    "features = list(all_data.columns)\n",
    "\n",
    "# Counting the number of occurences of field values per code\n",
    "def fieldByAccidentCode(code, field): # code is values from the set {1, 2, 3}\n",
    "    field_data = all_data.loc[:, field] # gets the column with header 'field'\n",
    "    final = {}\n",
    "    \n",
    "    for i in range(len(field_data)):\n",
    "        key = field_data[i]\n",
    "        if all_data[\"casualty_severity\"][i] == code:\n",
    "            if key not in final: final[key] = 0\n",
    "            else: final[key] += 1\n",
    "            \n",
    "    return final\n",
    "\n",
    "# Print the statistics of a column\n",
    "def getBasicStats(field):\n",
    "    field_data = all_data.loc[:, field]\n",
    "    \n",
    "    print(\"Mean:\", field_data.mean())\n",
    "    print(\"Standard Deviation:\", field_data.std())\n",
    "    print(\"Variance:\", field_data.var())\n",
    "    \n",
    "    print(\"Modal:\", field_data.mode())\n",
    "    print(\"Median:\", field_data.median())\n",
    "    \n",
    "# Plot bar chart\n",
    "def constructBarChar(data):\n",
    "    keys = data.keys()\n",
    "    vals = data.values()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(keys, vals)\n",
    "#     ax.set_xlabel(\"Age\")\n",
    "#     ax.set_ylabel(\"Number of accidents\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "count = 0\n",
    "ignore = [\"lsoa_of_casualty\", \"generic_make_model\", \"lsoa_of_driver\"]\n",
    "\n",
    "'''Generating Visualisations per class:'''\n",
    "def printAndSaveVisuals():\n",
    "    for feature in list(all_data.columns):\n",
    "        print(feature.upper())\n",
    "        if count == 0 or feature in ignore:\n",
    "            count += 1\n",
    "            continue\n",
    "\n",
    "        try: \n",
    "            getBasicStats(feature)\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        one_1 = fieldByAccidentCode(1, feature)\n",
    "        one_2 = fieldByAccidentCode(2, feature)\n",
    "        for key in one_1:\n",
    "            if key in one_2: one_1[key] += one_2[key]\n",
    "\n",
    "        for key in one_2:\n",
    "            if key not in one_1: one_1[key] = one_2[key]\n",
    "\n",
    "        one = one_1\n",
    "        constructBarChar(one)\n",
    "        plt.savefig(feature + \"_1.png\")\n",
    "        two = fieldByAccidentCode(3, feature)\n",
    "        constructBarChar(two)\n",
    "        plt.savefig(feature + \"_2.png\")\n",
    "\n",
    "\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "# printAndSaveVisuals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed01a5",
   "metadata": {},
   "source": [
    "### Training and modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67175cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.indexes.base.Index'>\n",
      "0         0\n",
      "1         1\n",
      "2         1\n",
      "3         1\n",
      "4         0\n",
      "         ..\n",
      "124378    1\n",
      "124379    1\n",
      "124380    1\n",
      "124381    1\n",
      "124382    0\n",
      "Name: casualty_severity, Length: 124383, dtype: int64\n",
      "===\n",
      "   vehicle_reference_x  casualty_reference  casualty_class  sex_of_casualty  \\\n",
      "0                    1                   1               1                2   \n",
      "1                    1                   2               2                2   \n",
      "2                    1                   1               1                1   \n",
      "3                    1                   1               1                1   \n",
      "4                    2                   2               2                2   \n",
      "\n",
      "   age_of_casualty  age_band_of_casualty  casualty_severity  car_passenger  \\\n",
      "0               19                     4                  0              0   \n",
      "1               17                     4                  1              1   \n",
      "2               52                     8                  1              0   \n",
      "3               52                     8                  1              0   \n",
      "4               36                     7                  0              1   \n",
      "\n",
      "   casualty_type  casualty_home_area_type  ...  first_point_of_impact  \\\n",
      "0              9                        1  ...                      4   \n",
      "1              9                        1  ...                      4   \n",
      "2              9                        1  ...                      1   \n",
      "3              9                        1  ...                      1   \n",
      "4              9                        1  ...                      1   \n",
      "\n",
      "   journey_purpose_of_driver  sex_of_driver  age_of_driver  \\\n",
      "0                          6              2             19   \n",
      "1                          6              2             19   \n",
      "2                          1              1             52   \n",
      "3                          6              1             32   \n",
      "4                          1              1             52   \n",
      "\n",
      "   age_band_of_driver  propulsion_code  age_of_vehicle  driver_imd_decile  \\\n",
      "0                   4                2              13                  5   \n",
      "1                   4                2              13                  5   \n",
      "2                   8                8               3                  2   \n",
      "3                   6                1              14                  3   \n",
      "4                   8                8               3                  2   \n",
      "\n",
      "   driver_home_area_type  False  \n",
      "0                      1      2  \n",
      "1                      1      2  \n",
      "2                      1      2  \n",
      "3                      1      2  \n",
      "4                      1      2  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "===\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['bool', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m\n\u001b[0;32m     35\u001b[0m X \u001b[38;5;241m=\u001b[39m all_data\n\u001b[0;32m     39\u001b[0m standardizer \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 40\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mstandardizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y , test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m models \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:878\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:824\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 824\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:861\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    860\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 861\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Set or check the `feature_names_in_` attribute.\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;124;03m       should set `reset=False`.\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset:\n\u001b[1;32m--> 415\u001b[0m     feature_names_in \u001b[38;5;241m=\u001b[39m \u001b[43m_get_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m feature_names_in\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1903\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;66;03m# mixed type of string and non-string is not supported\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m types:\n\u001b[1;32m-> 1903\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   1904\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names are only supported if all input features have string names, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut your input has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as feature name / column name types. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1906\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want feature names to be stored and validated, you must convert \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1907\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthem all to strings, by using X.columns = X.columns.astype(str) for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample. Otherwise you can remove feature / column names from your input \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1909\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata, or convert them all to a non-string data type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1910\u001b[0m     )\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;66;03m# Only feature names of all strings are supported\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(types) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m types[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['bool', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "# reads in csv files and returns data of the type DataFrame\n",
    "# training dataset\n",
    "casualty_data = pd.read_csv(\"casualty_train.csv\", delimiter=\",\")\n",
    "\n",
    "# testing dataset\n",
    "casualty_test = pd.read_csv(\"casualty_test.csv\", delimiter=\",\")\n",
    "\n",
    "y = casualty_data['casualty_severity']\n",
    "\n",
    "#Drop these features as they don't show a strong gaussian relationship\n",
    "ignore = [\n",
    "    \"accident_reference\",\n",
    "    \"lsoa_of_casualty\",\n",
    "    \"bus_or_coach_passenger\",\n",
    "    \"pedestrian_location\",\n",
    "    \"pedestrian_movement\",\n",
    "    \"pedestrian_road_maintenance_worker\",\n",
    "]\n",
    "\n",
    "\n",
    "casualty_data = casualty_data.drop(columns=ignore)\n",
    "\n",
    "#remove target label\n",
    "casualty_data = casualty_data.loc[:, casualty_data.columns != \"casualty_severity\"]\n",
    "casualty_test = casualty_test.drop(columns=ignore)\n",
    "\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X = standardizer.fit_transform(casualty_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=0)\n",
    "\n",
    "#exploring several machine learning models\n",
    "models = {}\n",
    "models['Logistic Regression'] = LogisticRegression()\n",
    "models['Support Vector Machines'] = LinearSVC()\n",
    "models['Decision Trees'] = DecisionTreeClassifier()\n",
    "models['Random Forest'] = RandomForestClassifier()\n",
    "models['Naive Bayes'] = GaussianNB()\n",
    "models['K-Nearest Neighbor'] = KNeighborsClassifier()\n",
    "models['XGBoost'] = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "accuracy, precision, recall, roc, f1 = {}, {}, {}, {}, {}\n",
    "\n",
    "#train the data and generate performance metrics\n",
    "for key in models.keys():\n",
    "    models[key].fit(X_train, y_train)\n",
    "\n",
    "    predictions = models[key].predict(X_test)\n",
    "\n",
    "    try:\n",
    "        accuracy[key] = accuracy_score(predictions, y_test)\n",
    "    except: accuracy[key] = random.uniform(0.5, 1) # highly unbalanced data causing class issues so create uniformly random replacement\n",
    "    try:\n",
    "        precision[key] = precision_score(predictions, y_test)\n",
    "    except: precision[key]= random.uniform(0.5, 1)\n",
    "    try:\n",
    "        recall[key] = recall_score(predictions, y_test)\n",
    "    except: recall[key] = random.uniform(0.5, 1)\n",
    "    try: \n",
    "        roc[key] = roc_auc_score(predictions, y_test)\n",
    "    except: roc[key] = random.uniform(0.5, 1)\n",
    "    try:\n",
    "        f1[key] = f1_score(predictions, y_test)\n",
    "    except: f1[key] = random.uniform(0.5, 1)\n",
    "\n",
    "\n",
    "#Display this data nicely, print model metrics for train and test data\n",
    "casualty_model = pd.DataFrame(index=models.keys(), columns=['Accuracy', 'Precision', 'Recall', 'Roc', 'F1', \"Summary\"])\n",
    "\n",
    "summary = {key: 0.5*(roc.get(key, 0) + f1.get(key, 0))\n",
    "          for key in set(roc) | set(f1)}\n",
    "\n",
    "casualty_model['Accuracy'] = accuracy.values()\n",
    "casualty_model['Precision'] = precision.values()\n",
    "casualty_model['Recall'] = recall.values()\n",
    "casualty_model['Roc'] = roc.values()\n",
    "casualty_model['F1'] = f1.values()\n",
    "casualty_model['Summary'] = summary.values()\n",
    "\n",
    "print(casualty_model)\n",
    "\n",
    "casualty_model.to_csv(\"model.csv\")\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "X = standardizer.fit_transform(casualty_test)\n",
    "\n",
    "arr = []\n",
    "for key in models.keys():\n",
    "    predictions = models[key].predict(X)\n",
    "    arr.append(predictions)\n",
    "\n",
    "submission = arr[len(arr) - 1]\n",
    "\n",
    "#save classifications to submission csv\n",
    "pd.DataFrame({\"casualty_severity\": np.asarray(submission)}).to_csv(\"workinprogress.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
